{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1f6724-1abe-44a9-a62e-682854da8bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introdução à Feature Store com Databricks Unity Catalog\n",
    "\n",
    "A Feature Store é um repositório centralizado de features para utilização em treinamento e inferência de modelos de machine learning, garantindo consistência e escalabilidade. Os principais benefícios de utilizá-la são:\n",
    "\n",
    "- Compartilhamento de features.\n",
    "- Reutilização e rastreabilidade aprimoradas.\n",
    "- Consistência no cálculo de features para treinamento e inferência.\n",
    "\n",
    "## Conteúdo deste Notebook\n",
    "\n",
    "Este notebook cobre as principais etapas para utilização da Feature Store para treinamento de modelos de machine learning.\n",
    "\n",
    "- Ingestão de dados e salvamento como uma feature table no Unity Catalog.\n",
    "- Carregamento de Features.\n",
    "- Criação de dataset de treinamento a partir de uma Feature Store.\n",
    "- Treinamento de um modelo simples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8560d5dc-3cc3-4040-bb5f-52ed12f87c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Instalar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d4c24f0-d78e-4e7b-ab19-12b2feec837f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering==0.7.0 databricks-sdk==0.20.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db762a2-45e7-46b5-b2de-4880bf390e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed4b0f2-df93-4421-9e59-876bf0cdb8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b846d86-e544-4d9e-869f-1987e9cd28b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "    sha2(concat(dropoff_zip, '-', pickup_zip, '-', tpep_pickup_datetime), 256) as trip_sk\n",
    "    , dropoff_zip\n",
    "    , pickup_zip\n",
    "    , date_part('month', tpep_pickup_datetime) as trip_month\n",
    "    , date_part('day', tpep_pickup_datetime) as trip_day\n",
    "    , date_part('dayofweek', tpep_pickup_datetime) as trip_dow\n",
    "    , date_part('hour', tpep_pickup_datetime) as trip_pickup_hour\n",
    "    , datediff(minute, tpep_pickup_datetime, tpep_dropoff_datetime) as trip_duration\n",
    "    , trip_distance\n",
    "    , fare_amount\n",
    "    FROM samples.nyctaxi.trips\n",
    "\"\"\"\n",
    "\n",
    "ft_nyc_taxi_trips = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d06082a-f83d-4cda-b204-4c3c6e7b1f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podemos utilizar a Pandas API para visualizar os resultados\n",
    "df = ft_nyc_taxi_trips.pandas_api()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3885fad-41a3-4451-a43f-c0853068ed1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Para evitar o vazamendo de dados realiza-se o drop da variável alvo\n",
    "df = df.drop(\"fare_amount\", axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94763788-5f40-4f67-a5ce-ea5d327f9e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvar a tabela de features\n",
    "Em seguida, vamos salvar nossa tabela de features como uma Tabela de Engenharia de Features utilizando o método `create_table`.\n",
    "\n",
    "Será necessário atribuir um nome à tabela e definir uma chave primária que será usada para consultas. A chave primária deve ser única."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc896e1-2b5e-4b71-9560-620ec3c98922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "    name=\"<catalog>.<schema>.fs_nyc_taxi_trips\",\n",
    "    primary_keys=[\"trip_sk\"],\n",
    "    df=df.to_spark(),\n",
    "    description=\"Taxi trips features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257af660-f5a0-41ec-8206-635adf56a365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Acessar metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28727b2c-d9ca-43c1-959d-2f34f58bfa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_get_table = fe.get_table(name=\"<catalog>.<schema>.fs_nyc_taxi_trips\")\n",
    "print(\"A feature store contém as features: \", fe_get_table.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ba74d6-4dff-4e0c-ae83-a28c189f6025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Treinando um modelo a partir da Feature Store \n",
    "\n",
    "### Construção do Dataset de Treinamento\n",
    "\n",
    "Iniciamos a construção do dataset de treinamento selecionando os IDs e a variável-alvo diretamente da tabela de origem. \n",
    "\n",
    "#### Etapas:\n",
    "\n",
    "1. Selecionamos as colunas relevantes:\n",
    "   - **`trip_sk`**: O identificador único para cada registro (*primary key*).\n",
    "   - **`fare_amount`**: A variável-alvo que será prevista pelo modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f813e02-9f60-4dcd-8715-9a7cabfaafc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spine_table = ft_nyc_taxi_trips.select(\"trip_sk\", \"fare_amount\")\n",
    "display(spine_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ae0bc6-27a6-4a49-8a47-9424b8ac61c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criando o Conjunto de Treinamento com Feature Lookup\n",
    "\n",
    "Agora, utilizando os IDs, podemos resgatar as features previamente criadas e disponibilizadas na Feature Store através de **Feature Lookup**. Em seguida, criamos o conjunto de treinamento utilizando o método `create_training_set` do **FeatureEngineeringClient**.\n",
    "\n",
    "#### Etapas:\n",
    "\n",
    "1. Definimos os *lookups* para as features que queremos usar no treinamento. \n",
    "   - Utilizamos o nome da tabela na Feature Store (`table_name`) e a chave de correspondência (`lookup_key`) que será utilizada para unir os dados originais com as features.\n",
    "   \n",
    "2. Criamos o conjunto de treinamento (*training set*) unindo o DataFrame original (`spine_table`) com os dados da Feature Store. Também definimos a variável-alvo (label) que será usada no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e7e3ad-4c1c-4373-abc9-2a32d75a5f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_feature_lookups = [\n",
    "      FeatureLookup(\n",
    "          table_name=\"<catalog>.<schema>.fs_nyc_taxi_trips\",\n",
    "          lookup_key=[\"trip_sk\"],\n",
    "      )\n",
    "]\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "    df=spine_table, \n",
    "    feature_lookups=model_feature_lookups,\n",
    "    label='fare_amount',\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "training_pd = training_set.load_df().toPandas()\n",
    "training_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccbab23-3230-4d5c-b5ad-5e4fc9d3d869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dividir o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f35b39-7d93-4632-9d17-2b14d4475319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target=\"fare_amount\"\n",
    "training_pd = training_pd.set_index(\"trip_sk\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_pd.drop(target, axis=1), training_pd[target], test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4882c7-e49b-4b80-ad8c-12020b0ef2f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89814b18-fdfa-4382-8096-512d92a8ae7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.XGBRegressor(enable_categorical=True)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "              \n",
    "# Print the metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Feature Store Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}