{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19a471e7-29af-4c5e-9570-b0243bf1c921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks: Usando Diferentes Linguagens (PySpark, SQL e Scala)\n",
    "\n",
    "Este notebook demonstra como usar diferentes linguagens dentro do Databricks para manipulação e análise de dados. Exploraremos PySpark, SQL e Scala, passando por exemplos práticos e mostrando como combinar essas linguagens em um mesmo fluxo de trabalho.\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "O Databricks oferece suporte a múltiplas linguagens dentro de um mesmo notebook, como PySpark (Python), SQL e Scala. Cada uma dessas linguagens tem suas vantagens e pode ser utilizada dependendo do contexto da operação que você deseja realizar.\n",
    "\n",
    "### PySpark:\n",
    "- Python é amplamente utilizado e tem uma curva de aprendizado mais baixa.\n",
    "- Recomendado para transformações e análises exploratórias rápidas.\n",
    "\n",
    "### SQL:\n",
    "- Linguagem declarativa ideal para consultas estruturadas e análises em grandes conjuntos de dados.\n",
    "- Útil para profissionais que já têm experiência com SQL.\n",
    "\n",
    "### Scala:\n",
    "- Linguagem nativa do Apache Spark. Oferece melhor performance para grandes volumes de dados e operações mais complexas.\n",
    "- Ideal para pipelines exigentes em termos de performance.\n",
    "\n",
    "Vamos ver exemplos de uso para cada uma dessas linguagens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b90923b-c2a4-4d97-b619-e7217afae1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 2. PySpark: Operações em Dados\n",
    "\n",
    "### 2.1 Carregando Dados com PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1740b876-0932-4412-ad39-5d46f0f6e262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Carregar o conjunto de dados Iris como um DataFrame Pandas\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.frame\n",
    "\n",
    "# Converter o DataFrame Pandas para um DataFrame Spark\n",
    "spark_df = spark.createDataFrame(iris_df)\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "478a45d0-14b2-4005-9259-cdc039bfcb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Filtrando e Agregando Dados em PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca3ca81-7d77-4005-b39c-0822ec8d54fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrando registros onde a coluna 'sepal width' é maior que 5.0\n",
    "df_filtered = spark_df.filter(spark_df[\"sepal width (cm)\"] > 1.5)\n",
    "\n",
    "# Agregando dados: calculando a média de 'sepal width' para cada valor alvo\n",
    "df_grouped = df_filtered.groupBy(\"target\").agg({\"sepal width (cm)\": \"mean\"})\n",
    "\n",
    "# Exibindo o resultado\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc218c43-2732-445a-8c4d-aeea9f872891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. SQL: Consultas em Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b28a85ca-345b-487c-b5cc-0ac7e5c454aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Criando uma Tabela Temporária e Realizando Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e58ec2-5eda-4f34-8e96-f870f0bc8316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando uma tabela temporária a partir do DataFrame PySpark\n",
    "spark_df.createOrReplaceTempView(\"tabela_iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae37712-bce2-4821-aef7-0da1986d3475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    target, \n",
    "    AVG(`sepal width (cm)`) AS avg_sepal_width\n",
    "FROM tabela_iris\n",
    "WHERE `sepal width (cm)` > 1.5\n",
    "GROUP BY target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576bde43-c317-4e95-9327-1d46edca9f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> O SQL permite fazer consultas de maneira declarativa. No exemplo acima, criamos uma tabela temporária a partir de um DataFrame PySpark e executamos uma consulta para filtrar dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f39ddb-6e8c-4697-958a-8f020504a941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107717d2-f431-4ab3-9dba-bfd5a6eadec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Consultando Dados com SQL em Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436dd35d-20da-401c-a1c6-85600533f03c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Lendo os dados do DataFrame no Scala (convertendo da tabela temporária criada no PySpark)\n",
    "val df = spark.sql(\"SELECT * FROM tabela_iris\")\n",
    "\n",
    "// Filtrando os registros onde 'sepal width (cm)' > 1.5\n",
    "val dfFiltered = df.filter($\"`sepal width (cm)`\" > 1.5)\n",
    "\n",
    "// Agrupando por 'target' e calculando a média de 'sepal width (cm)'\n",
    "import org.apache.spark.sql.functions._\n",
    "val dfGrouped = dfFiltered.groupBy(\"target\")\n",
    "  .agg(avg(\"sepal width (cm)\").alias(\"avg_sepal_width\"))\n",
    "\n",
    "// Exibindo o resultado\n",
    "dfGrouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d05b918-3e90-4c7c-b740-9f036b2e4409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> Você pode combinar SQL com Scala no Databricks. Isso permite fazer consultas estruturadas e depois manipular os resultados em Scala para otimizações adicionais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f3531f-6ae6-4faf-8264-292e171370ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4234149322913276,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Languages Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}