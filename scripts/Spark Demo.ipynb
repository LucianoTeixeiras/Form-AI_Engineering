{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7f57de-485b-4678-895c-bc084f904fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introdução ao Spark e Computação Distribuída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4ee99c-54fe-4be4-9227-99411f7faf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O Apache Spark é um sistema de computação distribuída de código aberto, projetado para lidar com o processamento de grandes volumes de dados de forma eficiente.\n",
    "\n",
    "Comparado a ferramentas tradicionais como o Pandas, o Spark é muito mais escalável para conjuntos de dados grandes que não cabem na memória de uma única máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d2ce76-6632-4244-8ccc-91f330270aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandas vs Spark\n",
    "\n",
    "O Pandas opera na memória (in-memory), o que significa que ele armazena e processa dados em uma única máquina.\n",
    "Isso funciona bem para conjuntos de dados pequenos ou médios, mas pode levar a limitações de memória ao lidar com grandes volumes de dados que não cabem na memória.\n",
    "\n",
    "O Spark, por outro lado, foi projetado para computação distribuída. Ele divide grandes conjuntos de dados em pedaços chamados partições,\n",
    "e processa essas partições em paralelo em múltiplos nós de um cluster. Essa arquitetura distribuída permite que o Spark lide com grandes volumes de dados de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57df80f-6eb7-44e3-88cb-290729ac64f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualizando as diferenças:\n",
    "- Pandas --> Processamento em Memória, em uma única máquina.\n",
    "- Spark   --> Processamento Paralelo, Distribuído em um Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f0b5eb-6863-474c-9c26-533441148da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Carregar o conjunto de dados Iris como um DataFrame Pandas\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.frame\n",
    "\n",
    "# Converter o DataFrame Pandas para um DataFrame Spark\n",
    "spark_df = spark.createDataFrame(iris_df)\n",
    "spark_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82130059-9c45-4deb-83dc-7fa0db89c118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Avaliação Preguiçosa (Lazy Evaluation)\n",
    "- Uma das otimizações de desempenho chave do Spark é a avaliação preguiçosa (lazy evaluation). Quando você define uma transformação em um DataFrame ou RDD do Spark, o Spark não executa imediatamente a operação.\n",
    "- Em vez disso, ele constrói uma DAG de transformações e só as executa quando uma ação (por exemplo, `count()`, `collect()`, `show()`) é chamada.\n",
    "- Isso permite que o Spark otimize todo o plano de execução antes de rodá-lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac22c50d-873c-458c-8af6-e06d368c3134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ---- Transformações preguiçosas no Spark ----\n",
    "# Aplicar um filtro no Spark (nenhuma computação acontece ainda)\n",
    "spark_lazy_transformation = spark_df.filter(col('sepal length (cm)') > 5.0)  # Nenhuma execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3debfa15-f6a2-4937-b025-efd115c45979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Somente quando uma ação é chamada (por exemplo, count ou show), o Spark realiza o trabalho\n",
    "spark_lazy_transformation.count() # Ação que dispara a execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bb7c85-5ae9-429b-95ba-1afd962b81f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podemos observar através do método ```.explain()``` o plano de execução que é criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9bb4c9-563e-49a1-99cf-93c2f80943b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "# ---- Otimização no Spark ----\n",
    "# Aplicar um filtro e depois agrupar, com plano de execução otimizado\n",
    "processed_spark_df = (\n",
    "    spark_df\n",
    "    .filter(col('sepal length (cm)') > 5.0)  # Filtragem antecipada para reduzir dados\n",
    "    .groupBy('target')\n",
    "    .agg({\"sepal length (cm)\": \"avg\"})  # Agrupar e calcular a média\n",
    ")\n",
    "\n",
    "# Exibir o plano de execução otimizado pelo Catalyst\n",
    "print(\"Plano de execução otimizado pelo Catalyst:\")\n",
    "processed_spark_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c52ac755-6ed7-4f40-81f3-5542418ce973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Explicação do Plano de Execução no Spark\n",
    "\n",
    "#### **1. Parsed Logical Plan**\n",
    "- Este é o **plano lógico inicial**, representado antes da resolução de nomes e tipos:\n",
    "  - Define um **agrupamento** por `target`.\n",
    "  - Inclui uma **agregação não resolvida** para a média de `sepal length (cm)` (`unresolvedalias('avg(sepal length (cm)#32)`).\n",
    "  - Mostra o **filtro** `sepal length (cm) > 5.0`.\n",
    "  - A entrada é um `LocalRelation`, que contém as colunas disponíveis: `sepal length (cm)`, `sepal width (cm)`, `petal length (cm)`, `petal width (cm)`, e `target`.\n",
    "\n",
    "#### **2. Analyzed Logical Plan**\n",
    "- Após a resolução de colunas e tipos de dados:\n",
    "  - `avg(sepal length (cm))` é resolvido como um alias para a agregação `avg(sepal length (cm)#32)`.\n",
    "  - As colunas envolvidas são:\n",
    "    - `target` (bigint).\n",
    "    - `avg(sepal length (cm))` (double).\n",
    "  - Mantém o **filtro** `sepal length (cm) > 5.0` antes do **agrupamento**.\n",
    "\n",
    "#### **3. Optimized Logical Plan**\n",
    "- Após as otimizações do Catalyst:\n",
    "  - Remove colunas irrelevantes do plano (`sepal width (cm)`, `petal length (cm)`, `petal width (cm)`), mantendo apenas `sepal length (cm)` e `target`.\n",
    "  - Aplica o **filtro** antes do agrupamento, reduzindo o volume de dados processados.\n",
    "  - A entrada permanece como um `LocalRelation`, indicando que os dados são locais e já carregados na memória.\n",
    "\n",
    "#### **4. Physical Plan**\n",
    "- Este é o plano executável no cluster Spark:\n",
    "  - **AdaptiveSparkPlan**:\n",
    "    - Indica que a execução adaptativa está habilitada, ajustando o plano físico durante a execução para otimização.\n",
    "  - **HashAggregate**:\n",
    "    - Divide o cálculo da média em duas etapas:\n",
    "      - **Agregação Parcial**: Calcula soma (`sum`) e contagem (`count`) localmente em cada partição.\n",
    "      - **Agregação Final**: Mescla os resultados das partições para calcular a média final.\n",
    "  - **Exchange**:\n",
    "    - Redistribui os dados por hash com base na coluna `target`, garantindo que os dados com o mesmo valor de `target` fiquem na mesma partição.\n",
    "  - **LocalTableScan**:\n",
    "    - Lê apenas as colunas necessárias (`sepal length (cm)` e `target`) diretamente da relação local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235ac3d9-d112-44f8-9fae-a5347d4a1a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- No Pandas, as operações são executadas imediatamente, enquanto no Spark, a execução é adiada até que uma ação seja chamada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a59c1bd-5ba3-4b32-b818-dad13725ad45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f555de-6b54-4d56-8f6b-3788377e304a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comparações Pandas e Spark DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d42564-137d-4e7f-9904-7c9b23a47c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importar Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ca14ba-96d3-46db-a397-241ae374fa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inicializar a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"PandasxSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb78661-2d7c-439d-9e77-c91e293d1a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Carregar Dados diretamente com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7e579e-3852-4dac-a7c4-b79d47a1f443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvar o dataset Iris como um arquivo CSV temporário no Databricks\n",
    "dbutils.fs.mkdirs(\"/tmp/iris_data\")  # Criar diretório temporário no DBFS\n",
    "iris_df.to_csv(\"/dbfs/tmp/iris_data/iris.csv\", index=False)  # Salvar o CSV no DBFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4e8a95-c6af-47da-969e-5fd15e3a9614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar o CSV diretamente como um DataFrame Spark\n",
    "spark_iris_from_csv = spark.read.csv(\"/tmp/iris_data/iris.csv\", header=True, inferSchema=True)\n",
    "spark_iris_from_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23448fdc-3804-433d-a870-54111c8f12d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Seleção de Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7829eb7e-575d-4293-9b57-93a8cc79aaf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Pandas ----\n",
    "# Selecionar colunas 'sepal length (cm)' e 'sepal width (cm)' usando Pandas\n",
    "pandas_selected = iris_df[['sepal length (cm)', 'sepal width (cm)']]\n",
    "print(\"Pandas - Colunas Selecionadas:\")\n",
    "pandas_selected.head() # Exibir as primeiras 5 linhas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28ec557-f697-4ae6-bb09-c2200f2e2a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Spark ----\n",
    "# Selecionar colunas 'sepal length (cm)' e 'sepal width (cm)' usando Spark\n",
    "spark_selected = spark_df.select('sepal length (cm)', 'sepal width (cm)')\n",
    "print(\"Spark - Colunas Selecionadas:\")\n",
    "spark_selected.show(5)  # Exibir as primeiras 5 linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d08d9f3-efd9-4b9e-b6e8-fa249ca43ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtrar Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186c43bd-7630-4720-acc4-c8db9a955b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar linhas onde 'sepal length (cm)' > 5.0\n",
    "pandas_filtered = iris_df[iris_df['sepal length (cm)'] > 5.0]\n",
    "print(\"Pandas - Linhas Filtradas (sepal length (cm) > 5.0):\")\n",
    "pandas_filtered.head()  # Exibir as primeiras 5 linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d953ca38-d49a-48aa-bd21-bbe26b1942e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Spark ----\n",
    "# Filtrar linhas onde 'sepal length (cm)' > 5.0\n",
    "spark_filtered = spark_df.filter(col('sepal length (cm)') > 5.0)\n",
    "print(\"Spark - Linhas Filtradas (sepal length (cm) > 5.0):\")\n",
    "spark_filtered.show(5)  # Exibir as primeiras 5 linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92435842-b771-4d3c-88d1-078c4bc59af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Agrupamento e Agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852f2bbb-521f-4024-9c57-1263e243885d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Pandas ----\n",
    "# Agrupar pelo target e calcular a média de 'sepal length (cm)'\n",
    "pandas_grouped = iris_df.groupby('target').agg(mean_sepal_length=('sepal length (cm)', 'mean'))\n",
    "pandas_grouped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0249e964-8b51-4781-97a9-4542f955dc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Spark ----\n",
    "# Agrupar pelo target e calcular a média de 'sepal length (cm)'\n",
    "spark_grouped = spark_df.groupBy('target').agg({\"sepal length (cm)\": \"avg\"})\n",
    "spark_grouped.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0fe0d1-6be4-40f8-8413-396737ced9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adicionar Novas Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a7470-0aca-401c-a162-24f9719ee342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ---- Pandas ----\n",
    "# Adicionar uma nova coluna que calcula a razão entre 'sepal length (cm)' e 'sepal width (cm)'\n",
    "iris_df['sepal_ratio'] = iris_df['sepal length (cm)'] / iris_df['sepal width (cm)']\n",
    "print(\"Pandas - Nova Coluna (sepal_ratio):\")\n",
    "iris_df[['sepal length (cm)', 'sepal width (cm)', 'sepal_ratio']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9be88b-35d9-4421-b3d0-29a68ba8df4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Spark ----\n",
    "# Adicionar uma nova coluna que calcula a razão entre 'sepal length (cm)' e 'sepal width (cm)'\n",
    "spark_df = spark_df.withColumn('sepal_ratio', col('sepal length (cm)') / col('sepal width (cm)'))\n",
    "\n",
    "# Mostrar que a avaliação é preguiçosa no Spark\n",
    "print(\"Spark - Nova Coluna (sepal_ratio):\")\n",
    "spark_df.select('sepal length (cm)', 'sepal width (cm)', 'sepal_ratio').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9014489-6367-450d-9e87-3dbfec0abcd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converter Spark DataFrame para Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cb9ed9-3348-4ff9-ac52-37e2c1124358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converter DataFrame Spark para Pandas\n",
    "pandas_from_spark = spark_df.toPandas()\n",
    "\n",
    "pandas_from_spark.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6241c81b-7a4e-4644-87c9-f3ece16b1dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converter Pandas DataFrame para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b123d4f-13a6-46e4-b614-95cb12b8b720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar DataFrame Spark\n",
    "spark_from_pandas = spark.createDataFrame(iris_df)\n",
    "spark_from_pandas.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8b5b87-952d-426a-a6fc-932fdf822eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- O Pandas é excelente para conjuntos de dados pequenos e manipulações rápidas.\n",
    "\n",
    "- O Spark é essencial para operações em escala, lidando com grandes conjuntos de dados distribuídos por clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592eb048-f6c2-47bf-8e3d-82f5f1dbaf05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}